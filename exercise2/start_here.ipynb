{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the second exercise of the Machine Learning for Visual Computing (183.605) lecture at TU Wien. Assignment via TUWEL. Please be aware of the deadlines in TUWEL.\n",
    "\n",
    "* Upload a zip-file with the required programms. The programming language is python.\n",
    "    1. Gaussian Process --> `solutions/gaussian_process.py`\n",
    "    3. Vision Transformer --> `solutions/vision_transformer.py`\n",
    "* If you needed additional conda or pip packages add an anaconda environment.yml to the zip-file\n",
    "    1. conda env export > environment.yml\n",
    "    2. See --> https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#sharing-an-environment\n",
    "\n",
    "Important Note: In order to run the code error free, you need to install Latex locally. Alternatively, you can use JupyterHub, accessible via the TUWEL page.\n",
    "\n",
    "##### Conda instructions\n",
    "\n",
    "1. conda create --name MLVC-2\n",
    "4. conda activate MLVC-2\n",
    "5. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "6. python -m pip install pillow matplotlib tqdm torchinfo ipykernel ipywidgets scikit-learn seaborn einops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import rcParams\n",
    "from util.gaussian_process_util import (\n",
    "    plot_gp_results,\n",
    "    plot_mvn,\n",
    "    plot_new_visualization,\n",
    "    plot_conditioned_new_visualization,\n",
    "    plot_kernels,\n",
    "    generate_noisy_points,\n",
    ")\n",
    "from solution_G_X.gaussian_process import (\n",
    "    MultivariateNormal,\n",
    "    GaussianProcess,\n",
    "    sample_points,\n",
    ")\n",
    "import seaborn as sns\n",
    "\n",
    "# You might need to set the path such that the latex executable can be found\n",
    "# os.environ[\"PATH\"] += os.pathsep + \"/usr/local/bin\"\n",
    "\n",
    "rcParams[\"text.latex.preamble\"] = r\"\\usepackage{amsmath}\"\n",
    "rcParams[\"text.usetex\"] = True\n",
    "np.set_printoptions(suppress=True)\n",
    "# Set matplotlib and seaborn plotting style\n",
    "sns.set_style(\"darkgrid\")\n",
    "np.random.seed(42)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Imports\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process\n",
    "\n",
    "### Multivariate Normal Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0, 0]\n",
    "\n",
    "x = np.linspace(-2, 2, 81)\n",
    "y = np.linspace(-2, 2, 81)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "subfigs = fig.subfigures(1, 3, wspace=0.01)\n",
    "\n",
    "axis_left = subfigs[0].subplots()\n",
    "axis_mid = subfigs[1].subplots()\n",
    "axis_right = subfigs[2].subplots()\n",
    "\n",
    "cov = np.array([[1, 0], [0, 1]])\n",
    "distr = MultivariateNormal(cov=cov, mean=mean, seed=1000)\n",
    "plot_mvn(axis_left, mean, cov, distr)\n",
    "\n",
    "cov = np.array([[1, 0.5], [0.5, 1]])\n",
    "distr = MultivariateNormal(cov=cov, mean=mean, seed=1000)\n",
    "plot_mvn(axis_mid, mean, cov, distr)\n",
    "\n",
    "cov = np.array([[1, 0.9], [0.9, 1]])\n",
    "distr = MultivariateNormal(cov=cov, mean=mean, seed=1000)\n",
    "plot_mvn(axis_right, mean, cov, distr)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(255, 255, 255, 1); text-align:center; vertical-align: middle; padding:40px 0;\">\n",
    "\n",
    "#### <span style=\"color:blue\"> Example.</span>\n",
    "\n",
    "![h](img/3_multivariate_distributions.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Visualization (with independent gaussians)\n",
    "\n",
    "Randomly sample 50 points from the independent multivariate normal distribution and plot one of them using the new visualization technique. The selected point is plotted in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0, 0]\n",
    "cov = np.array([[1, 0], [0, 1]])\n",
    "\n",
    "distr = MultivariateNormal(cov=cov, mean=mean, seed=1000)\n",
    "x = np.linspace(-3 * cov[0, 0], 3 * cov[0, 0], num=200)\n",
    "y = np.linspace(-3 * cov[1, 1], 3 * cov[1, 1], num=200)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Generating the density function\n",
    "# for each point in the meshgrid\n",
    "pdf = distr.pdf(X, Y)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "subfigs = fig.subfigures(1, 2, wspace=0.01)\n",
    "\n",
    "axis_left = subfigs[0].subplots()\n",
    "axis_right = subfigs[1].subplots()\n",
    "\n",
    "sampled_points = sample_points(mean, cov, n=50)\n",
    "\n",
    "plot_mvn(axis_left, mean, cov, distr, sampled_points=sampled_points)\n",
    "plot_new_visualization(axis_right, sampled_points)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(255, 255, 255, 1); text-align:center; vertical-align: middle; padding:40px 0;\">\n",
    "\n",
    "#### <span style=\"color:blue\"> Example.</span>\n",
    "\n",
    "![h](img/new_viz_independent.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Visualization (with dependent gaussians)\n",
    "\n",
    "Randomly sample 50 points from the dependent multivariate normal distribution and plot one of them using the new visualization technique. The selected point is plotted in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0, 0]\n",
    "cov = np.array([[1, 0.9], [0.9, 1]])\n",
    "\n",
    "distr = MultivariateNormal(cov=cov, mean=mean, seed=1000)\n",
    "x = np.linspace(-3 * cov[0, 0], 3 * cov[0, 0], num=200)\n",
    "y = np.linspace(-3 * cov[1, 1], 3 * cov[1, 1], num=200)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Generating the density function\n",
    "# for each point in the meshgrid\n",
    "pdf = distr.pdf(X, Y)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "subfigs = fig.subfigures(1, 2, wspace=0.01)\n",
    "\n",
    "axis_left = subfigs[0].subplots()\n",
    "axis_right = subfigs[1].subplots()\n",
    "\n",
    "sampled_points = sample_points(mean, cov, n=50)\n",
    "\n",
    "plot_mvn(axis_left, mean, cov, distr, sampled_points=sampled_points)\n",
    "plot_new_visualization(axis_right, sampled_points)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(255, 255, 255, 1); text-align:center; vertical-align: middle; padding:40px 0;\">\n",
    "\n",
    "#### <span style=\"color:blue\"> Example.</span>\n",
    "\n",
    "![h](img/new_viz_dependent.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditioned Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0, 0]\n",
    "cov = np.array([[1, 0.9], [0.9, 1]])\n",
    "\n",
    "distr = MultivariateNormal(cov=cov, mean=mean, seed=1000)\n",
    "x = np.linspace(-3 * cov[0, 0], 3 * cov[0, 0], num=601)\n",
    "y = np.linspace(-3 * cov[1, 1], 3 * cov[1, 1], num=601)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Generating the density function\n",
    "# for each point in the meshgrid\n",
    "pdf = distr.pdf(X, Y)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "subfigs = fig.subfigures(1, 3, wspace=0.01)\n",
    "\n",
    "axis_left = subfigs[0].subplots()\n",
    "axis_mid = subfigs[1].subplots()\n",
    "axis_right = subfigs[2].subplots()\n",
    "\n",
    "sampled_points = sample_points(mean, cov, n=50, observation=1.2)\n",
    "\n",
    "plot_mvn(axis_left, mean, cov, distr, sampled_points=sampled_points, conditioned=True)\n",
    "plot_conditioned_new_visualization(axis_mid, axis_right, sampled_points, pdf, X, y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(255, 255, 255, 1); text-align:center; vertical-align: middle; padding:40px 0;\">\n",
    "\n",
    "#### <span style=\"color:blue\"> Example.</span>\n",
    "\n",
    "![h](img/conditioned_gaussian.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Gaussian Process\n",
    "\n",
    "A Gaussian process is defined as a collection of random variables, any finite number of which have a joint Gaussian distribution. A GP is fully specified by its mean function $m(\\mathbf{x})$ and covariance function (kernel) $k(\\mathbf{x}, \\mathbf{x'})$:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "f(\\mathbf{x}) \\sim \\mathcal{GP}(m(\\mathbf{x}), k(\\mathbf{x}, \\mathbf{x'})) \n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "#### Kernel Functions\n",
    "\n",
    "The choice of kernel determines the smoothness and behavior of the GP. Common kernels include the Radial Basis Function (RBF), the Gaussian kernel or the Exponential Sinine kernel. The RBF kernel is defined as:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "k(\\mathbf{x}, \\mathbf{x'}) = \\exp(-\\frac{d(\\mathbf{x}, \\mathbf{x'})^2}{2l^2})\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "where $l$ is the lengthscale parameter and $d(\\cdot, \\cdot)$ is the Euclidean distance. The Exponential Sine kernel is defined as:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "k(\\mathbf{x}, \\mathbf{x'}) = \\exp(-\\frac{2\\sin^2(\\pi d(\\mathbf{x}, \\mathbf{x'}))}{l^2})\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "where $l$ is the lengthscale parameter and $d(\\cdot, \\cdot)$ is the Euclidean distance and $p$ is the periodicity of the kernel.\n",
    "\n",
    "#### Training\n",
    "\n",
    "Training involves optimizing the hyperparameters (e.g., lengthscale, noise variance, periodicity) to maximize the likelihood of the observed data under the GP. In this example these values are given, however, in the next example you will use the negative log marginal likelihood to optimize the hyperparameters. We assume the mean function is zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = \"Sine\"  # Default: \"Sine\", but \"RBF\" or \"Sine+RBF\" are also valid options\n",
    "n1 = 10  # Number of points to condition on (training points)\n",
    "n2 = 1000  # Number of points in posterior (test points)\n",
    "ny = 5  # Number of functions that will be sampled from the posterior\n",
    "domain = (0, 10)  # Domain bounds\n",
    "noise = 0.4  # Noise of the data\n",
    "\n",
    "\n",
    "# Define the true function that we want to regress on\n",
    "def f_sin(x):\n",
    "    return (np.sin(x * 3) * 2).flatten()\n",
    "\n",
    "\n",
    "# Sample observations (X1, y1) on the function\n",
    "Xtrain, t_train = generate_noisy_points(n1, noise, f_sin, domain, seed=27)\n",
    "# Predict points at uniform spacing to capture function\n",
    "Xtest = np.linspace(domain[0], domain[1], n2).reshape(-1, 1)\n",
    "\n",
    "gp = GaussianProcess(length_scale=0.3, noise=noise, kernel=kernel, periodicity=2)\n",
    "gp.fit(Xtrain, t_train, meta_parameter_search=False)\n",
    "# Compute posterior mean and covariance\n",
    "pred_distribution = gp.predict(Xtest)\n",
    "\n",
    "(\n",
    "    mean_pred_distribution,\n",
    "    std_pred_distribution,\n",
    "    conv_pred_distribution,\n",
    ") = pred_distribution\n",
    "\n",
    "# Draw some samples of the posterior\n",
    "ytest_pred = np.random.multivariate_normal(\n",
    "    mean=mean_pred_distribution, cov=conv_pred_distribution, size=ny\n",
    ")\n",
    "\n",
    "plot_gp_results(f_sin, Xtrain, Xtest, t_train, ytest_pred, domain, pred_distribution)\n",
    "\n",
    "plot_kernels(Xtrain, Xtest, gp.kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(255, 255, 255, 1); text-align:center; vertical-align: middle; padding:40px 0;\">\n",
    "\n",
    "#### <span style=\"color:blue\"> Example.</span>\n",
    "\n",
    "![h](img/cov.svg)\n",
    "\n",
    "#### <span style=\"color:blue\"> Predictions Gaussian Process.</span>.\n",
    "![h](img/gp_output.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Gaussian Process with negative log likelihood\n",
    "\n",
    "Optimal values for the kernel parameters can be estimated by maximizing the log marginal likelihood which is given by:\n",
    "$\n",
    "\\begin{align}\n",
    "log p(t|X) = log \\mathcal{N}(t|0,K_y) = -\\frac{1}{2}t^TK_{t}^{-1}t-\\frac{1}{2}log|K_t|-\\frac{N}{2}log(2\\pi)\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "In the following we will minimize the negative log marginal likelihood w.r.t. parameters $l$ and $σ_f$, $σ_y$ is set to the known noise level of the data. If the noise level is unknown, $σ_y$ can be estimated as well along with the other parameters. However, in this exercise the noise level is given. You may use a numerical optimization method of your choice (we recommend scipy.optimize and scipy.linalg: cho_solve, cholesky, solve_triangular)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = \"Sine\"  # Default: \"Sine\", but \"RBF\" or \"Sine+RBF\" are also valid options\n",
    "n1 = 10  # Number of points to condition on (training points)\n",
    "n2 = 1000  # Number of points in posterior (test points)\n",
    "ny = 5  # Number of functions that will be sampled from the posterior\n",
    "domain = (0, 10)  # Domain bounds\n",
    "noise = 0.4  # Noise of the data\n",
    "\n",
    "\n",
    "# Define the true function that we want to regress on\n",
    "def f_sin(x):\n",
    "    return (np.sin(x * 3) * 2).flatten()\n",
    "\n",
    "\n",
    "# Sample observations (X1, y1) on the function\n",
    "Xtrain, t_train = generate_noisy_points(n1, noise, f_sin, domain, seed=27)\n",
    "# Predict points at uniform spacing to capture function\n",
    "Xtest = np.linspace(domain[0], domain[1], n2).reshape(-1, 1)\n",
    "\n",
    "gp = GaussianProcess(length_scale=0.3, noise=noise, kernel=kernel, periodicity=2)\n",
    "# to predict only based on the prior, comment the fit call\n",
    "gp.fit(Xtrain, t_train, meta_parameter_search=True)\n",
    "# Compute posterior mean and covariance\n",
    "pred_distribution = gp.predict(Xtest)\n",
    "\n",
    "(\n",
    "    mean_pred_distribution,\n",
    "    std_pred_distribution,\n",
    "    conv_pred_distribution,\n",
    ") = pred_distribution\n",
    "\n",
    "# Draw some samples of the posterior\n",
    "ytest_pred = np.random.multivariate_normal(\n",
    "    mean=mean_pred_distribution, cov=conv_pred_distribution, size=ny\n",
    ")\n",
    "plot_gp_results(f_sin, Xtrain, Xtest, t_train, ytest_pred, domain, pred_distribution)\n",
    "\n",
    "plot_kernels(Xtrain, Xtest, gp.kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(255, 255, 255, 1); text-align:center; vertical-align: middle; padding:40px 0;\">\n",
    "\n",
    "#### <span style=\"color:blue\"> Example.</span>\n",
    "\n",
    "![h](img/cov_nll.svg)\n",
    "\n",
    "#### <span style=\"color:blue\"> Predictions Gaussian Process.</span>.\n",
    "![h](img/gp_output_nll.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Dataset to PyTorch\n",
    "\n",
    "In this section we first define some parameters like the number of training samples or the image size. Do the general imports and then we load the dataset and convert it to a PyTorch dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 10000  # Recommended: 10.000\n",
    "TEST_RATIO = 0.2  # Recommended: 0.2\n",
    "BATCH_SIZE = 64  # Recommended: 64\n",
    "IMAGE_SIZE = 32  # Recommended: 32 (but you can also try 16, 64 or 128, depending on your hardware)\n",
    "GET_ACCURACY_PER_EPOCH = False  # Recommended: False (takes a lot of time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from util.generate_dataset import make_dataset\n",
    "from util.attention_visualizer import Visualizer\n",
    "from solution_G_X.vision_transformer import VisionTransformer\n",
    "from util.vision_transformer_util import CircleSquareDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dataset_train, labels_train), (dataset_test, labels_test) = make_dataset(\n",
    "    NUM_SAMPLES, TEST_RATIO, IMAGE_SIZE\n",
    ")\n",
    "\n",
    "print(dataset_train.shape, labels_train.shape, dataset_test.shape, labels_test.shape)\n",
    "\n",
    "dataset_pytorch_train = CircleSquareDataset(\n",
    "    dataset_train, labels_train, image_size=IMAGE_SIZE\n",
    ")\n",
    "dataset_pytorch_test = CircleSquareDataset(\n",
    "    dataset_test, labels_test, image_size=IMAGE_SIZE\n",
    ")\n",
    "\n",
    "train_dataloader = data.DataLoader(\n",
    "    dataset_pytorch_train, shuffle=True, batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "test_dataloader = data.DataLoader(dataset_pytorch_test, shuffle=True, batch_size=1)\n",
    "\n",
    "fig, axs = plt.subplots(2, 8, figsize=(8, 2), dpi=200)\n",
    "axs = axs.reshape(-1)\n",
    "for i, (x, y) in enumerate(train_dataloader):\n",
    "    axs[i].imshow(x[0].sum(dim=0).reshape((IMAGE_SIZE, IMAGE_SIZE)), cmap=\"gray\")\n",
    "    axs[i].axis('off')\n",
    "\n",
    "    if i == 15:\n",
    "        break\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer (Encoder - Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check if a GPU is available and use it. To force CPU set: device = \"cpu\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transf = VisionTransformer(\n",
    "    image_size=IMAGE_SIZE,\n",
    "    patch_size=4,\n",
    "    num_classes=1,  # We don't predict classes\n",
    "    dim=16,\n",
    "    depth=2,\n",
    "    heads=4,\n",
    "    mlp_dim=64,\n",
    "    dropout=0.0\n",
    ")\n",
    "\n",
    "# Convert transformer to selected device\n",
    "transf = transf.to(device)\n",
    "\n",
    "transf.train()\n",
    "\n",
    "print(summary(transf, input_size=(BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE)))\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.AdamW(transf.parameters(), lr=0.001)\n",
    "loss_per_epoch = []\n",
    "acc_per_epoch = []\n",
    "for epoch in trange(30):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, (data_, label_) in enumerate(train_dataloader, 0):\n",
    "        # Convert data and label (target in lecture) to selected device\n",
    "        data_ = data_.to(device)\n",
    "        label_ = label_.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = transf(data_).squeeze()\n",
    "\n",
    "        loss = criterion(outputs, label_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    if GET_ACCURACY_PER_EPOCH:\n",
    "        transf.eval()\n",
    "        data_frame, acc = transf.predict(test_dataloader, device)\n",
    "        transf.train()\n",
    "        acc_per_epoch.append(acc)\n",
    "    else:\n",
    "        acc_per_epoch.append(0)\n",
    "\n",
    "    loss_per_epoch.append(running_loss / len(train_dataloader))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax[0].plot(np.arange(len(acc_per_epoch)) + 1, acc_per_epoch)\n",
    "ax[0].set_ylabel(\"Accuracy\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "\n",
    "ax[1].plot(np.arange(len(loss_per_epoch)) + 1, loss_per_epoch)\n",
    "ax[1].set_ylabel(\"Loss\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(255, 255, 255, 1); text-align:center; vertical-align: middle; padding:40px 0;\">\n",
    "\n",
    "#### <span style=\"color:blue\"> Potential transformer accuracy and loss.</span>\n",
    "![h](img/transformer_loss.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf.eval()\n",
    "data_frame, acc = transf.predict(test_dataloader, device)\n",
    "print(data_frame[0:20])\n",
    "print(\"Test ACC:\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = Visualizer(model=transf, patch_size=4)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (data_, label_) in enumerate(test_dataloader, 0):\n",
    "        data_ = data_.to(device)\n",
    "        label_ = label_.to(device)\n",
    "\n",
    "        visualizer.visualize_predict(data_)\n",
    "\n",
    "        if i == 2:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(255, 255, 255, 1); text-align:center; vertical-align: middle; padding:40px 0;\">\n",
    "\n",
    "#### <span style=\"color:blue\"> Example.</span>\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"img/example_viz_att_square.svg\" alt=\"Drawing\" style=\"width: 800px;\"/> </td>\n",
    "<td> <img src=\"img/example_viz_att_circle.svg\" alt=\"Drawing\" style=\"width: 800px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvc-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
